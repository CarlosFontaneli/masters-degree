{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "from torchtrainer.util.train_util import dict_to_argv\n",
    "\n",
    "# All samples in the VessMAP dataset\n",
    "names = [\n",
    "    '5472', '2413', '3406', '7577', '4404', '12005', '10084', '3882', '15577', '15375', '8353', '17035', \n",
    "    '13114', '4413', '7783', '11411', '6524', '6581', '13200', '9860', '525', '2643', '8990', '9284', \n",
    "    '2050', '2071', '13128', '7865', '14440', '8196', '17880', '1643', '11558', '12943', '2546', '9452', \n",
    "    '11828', '8493', '14225', '8256', '1816', '14121', '11161', '16707', '356', '12877', '6818', '10571', \n",
    "    '6672', '17702', '15821', '8429', '18180', '13528', '16689', '12960', '5359', '6384', '7392', '6887', \n",
    "    '8506', '1585', '4938', '458', '5801', '8686', '15160', '7413', '8065', '8284', '9593', '17584', '2849', \n",
    "    '9710', '5740', '4739', '2958', '14787', '11098', '17630', '11111', '6656', '17852', '9000', '12455', '9523', \n",
    "    '4909', '12618', '14778', '16295', '17425', '14690', '12749', '12335', '7083', '2287', '482', '7344', '18035', \n",
    "    '16766'\n",
    "]\n",
    "\n",
    "\n",
    "# Base parameters – these are the defaults for all models.\n",
    "base_params = {\n",
    "    # Logging parameters:\n",
    "    \"experiments_path\": \"/home/fonta42/Desktop/masters-degree/experiments/torch-trainer\",  \n",
    "    \"run_name\": \"\",  \n",
    "    \"validate_every\": 50,\n",
    "    \"val_img_indices\": \"0 1 2 3\",\n",
    "    \"copy_model_every\": 0,\n",
    "    \"wandb_project\": \"uncategorized\",\n",
    "\n",
    "    # Dataset parameters:\n",
    "    \"dataset_path\": \"/home/fonta42/Desktop/masters-degree/data/torch-trainer/VessMAP\",  \n",
    "    \"dataset_class\": \"vessmap_few\",\n",
    "    \"resize_size\": \"256 256\",        \n",
    "    \"loss_function\": \"bce\",          \n",
    "\n",
    "    # Model parameters:\n",
    "    \"model_class\": \"\",              \n",
    "\n",
    "    # Training parameters:\n",
    "    \"num_epochs\": 1000, \n",
    "    \"validation_metric\": \"Dice\",\n",
    "    \"lr\": 0.001, \n",
    "    \"lr_decay\": 1.0,\n",
    "    \"bs_train\": 2, \n",
    "    \"bs_valid\": 2,\n",
    "    \"weight_decay\": 0.0, # TODO: aumentar/variar em casos de overfitting\n",
    "    \"optimizer\": \"adam\", \n",
    "    \"momentum\": 0.9,\n",
    "    \"seed\": 42,\n",
    "\n",
    "    # Device and efficiency parameters:\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"num_workers\": 5,\n",
    "}\n",
    "\n",
    "# Define variations for the parameters we want to test:\n",
    "split_variations = [20, 90] # TODO:  variar nos extremos, 0.2 e rand 0.9, garantir que splits tenham tamanhos pares pro batch size bater\n",
    "val_img_indices_variations = [\"0 1 2 3\"]\n",
    "loss_function_variations = [\"bce\", \"cross_entropy\"]\n",
    "lr_variations = [0.001, 0.01]\n",
    "lr_decay_variations = [1.0, 0.9]\n",
    "weight_decay_variations = [0.0, 1e-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medsam_train_torchtrainer import MedsamTrainer\n",
    "\n",
    "# Override the resize_size and model_class for MedSAM\n",
    "medsam_base = copy.deepcopy(base_params)\n",
    "medsam_base[\"resize_size\"] = \"1024 1024\"\n",
    "medsam_base[\"model_class\"] = \"medsam\"\n",
    "medsam_base[\"experiment_name\"] = \"medsam_runs\"\n",
    "\n",
    "\n",
    "for split in split_variations:\n",
    "    for loss_func in loss_function_variations:\n",
    "        for lr in lr_variations:\n",
    "            for lr_decay in lr_decay_variations:\n",
    "                for wd in weight_decay_variations:\n",
    "                    params = copy.deepcopy(medsam_base)\n",
    "                    params['split_strategy'] = ','.join(random.sample(names, split))\n",
    "                    params[\"loss_function\"] = loss_func\n",
    "                    params[\"lr\"] = lr\n",
    "                    params[\"lr_decay\"] = lr_decay\n",
    "                    params[\"weight_decay\"] = wd\n",
    "                    # Build the run_name from key parameters:\n",
    "                    params[\"run_name\"] = f\"medsam_{split}_1024x1024_{loss_func}_10_{lr}_{params['bs_train']}_{params['bs_valid']}_{wd}\"\n",
    "                    \n",
    "                    print(\"Running MedSAM experiment with parameters:\")\n",
    "                    commandline = ' '.join(dict_to_argv(params, [\"dataset_path\", \"dataset_class\", \"model_class\"]))\n",
    "                    print(f\"python medsam_train_torchtrainer.py {commandline}\")\n",
    "                    #!python medsam_train_torchtrainer.py {commandline}\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet_train import UnetVessTrainer\n",
    "\n",
    "for split in split_variations:\n",
    "    for val_indices in val_img_indices_variations:\n",
    "            for loss_func in loss_function_variations:\n",
    "                for lr in lr_variations:\n",
    "                    for lr_decay in lr_decay_variations:\n",
    "                        for wd in weight_decay_variations:\n",
    "                            params = copy.deepcopy(base_params)\n",
    "                            params['split_strategy'] = ','.join(random.sample(names, split))\n",
    "                            params[\"val_img_indices\"] = val_indices\n",
    "                            params[\"loss_function\"] = loss_func\n",
    "                            params[\"lr\"] = lr\n",
    "                            params[\"lr_decay\"] = lr_decay\n",
    "                            params[\"weight_decay\"] = wd\n",
    "                            params[\"resize_size\"] = \"256 256\"\n",
    "                            params[\"model_class\"] = \"unet_smp\"\n",
    "                            params[\"run_name\"] = f\"unet_{split}_256x256_{loss_func}_{params['num_epochs']}_{lr}_{params['bs_train']}_{params['bs_valid']}_{wd}\"\n",
    "                            \n",
    "                            print(\"Running UNet experiment with parameters:\")\n",
    "                            print(params)\n",
    "                            commandline = ' '.join(dict_to_argv(params, [\"dataset_path\", \"dataset_class\", \"model_class\"]))\n",
    "\n",
    "                            !python unet_train_torchtrainer.py {commandline}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "import subprocess # Using subprocess is generally more robust than '!' for running scripts\n",
    "from torchtrainer.util.train_util import dict_to_argv # Assuming this function is available\n",
    "\n",
    "# --- Dataset Sample Names ---\n",
    "# All samples in the VessMAP dataset\n",
    "names = [\n",
    "    '5472', '2413', '3406', '7577', '4404', '12005', '10084', '3882', '15577', '15375', '8353', '17035',\n",
    "    '13114', '4413', '7783', '11411', '6524', '6581', '13200', '9860', '525', '2643', '8990', '9284',\n",
    "    '2050', '2071', '13128', '7865', '14440', '8196', '17880', '1643', '11558', '12943', '2546', '9452',\n",
    "    '11828', '8493', '14225', '8256', '1816', '14121', '11161', '16707', '356', '12877', '6818', '10571',\n",
    "    '6672', '17702', '15821', '8429', '18180', '13528', '16689', '12960', '5359', '6384', '7392', '6887',\n",
    "    '8506', '1585', '4938', '458', '5801', '8686', '15160', '7413', '8065', '8284', '9593', '17584', '2849',\n",
    "    '9710', '5740', '4739', '2958', '14787', '11098', '17630', '11111', '6656', '17852', '9000', '12455', '9523',\n",
    "    '4909', '12618', '14778', '16295', '17425', '14690', '12749', '12335', '7083', '2287', '482', '7344', '18035',\n",
    "    '16766'\n",
    "]\n",
    "\n",
    "# --- Base Parameters ---\n",
    "# These parameters are common defaults for all experiments.\n",
    "base_params = {\n",
    "    # Logging parameters:\n",
    "    \"experiments_path\": \"/home/fonta42/Desktop/masters-degree/experiments/torch-trainer\",\n",
    "    \"run_name\": \"\", # Will be dynamically generated per run\n",
    "    \"validate_every\": 50,\n",
    "    \"copy_model_every\": 0,\n",
    "    \"wandb_project\": \"uncategorized\",\n",
    "\n",
    "    # Dataset parameters:\n",
    "    \"dataset_path\": \"/home/fonta42/Desktop/masters-degree/data/torch-trainer/VessMAP\",\n",
    "    \"dataset_class\": \"vessmap_few\",\n",
    "    \"resize_size\": \"256 256\", # Default, can be overridden by model specifics\n",
    "    \"loss_function\": \"bce\", # Default, can be overridden by variations\n",
    "\n",
    "    # Model parameters:\n",
    "    \"model_class\": \"\", # To be set specifically for each model type\n",
    "\n",
    "    # Training parameters:\n",
    "    \"num_epochs\": 1000,\n",
    "    \"validation_metric\": \"Dice\",\n",
    "    \"lr\": 0.001, # Default, can be overridden by variations\n",
    "    \"lr_decay\": 1.0, # Default, can be overridden by variations\n",
    "    \"bs_train\": 2,\n",
    "    \"bs_valid\": 2,\n",
    "    \"weight_decay\": 0.0, # Default, can be overridden by variations\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"momentum\": 0.9,\n",
    "    \"seed\": 42,\n",
    "\n",
    "    # Device and efficiency parameters:\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"num_workers\": 5,\n",
    "    \"benchmark\": \"\", # Empty string likely means 'use benchmark if available' or default behavior\n",
    "}\n",
    "\n",
    "\n",
    "# --- Parameter Variations ---\n",
    "# Define the parameters to vary and their possible values.\n",
    "# A temporary key 'split_size' is used to represent the number of samples for the split.\n",
    "# The actual 'split_strategy' string will be generated dynamically for each run.\n",
    "parameter_variations = {\n",
    "    \"split_strategy\": [20, 90],  # Number of samples to select for the training split strategy\n",
    "    \"val_img_indices\": [\"0 1 2 3\", \"0 2 4\"], # Note: MedSAM loop might ignore this\n",
    "    \"loss_function\": [\"bce\", \"cross_entropy\"],\n",
    "    \"lr\": [0.001, 0.01],\n",
    "    \"lr_decay\": [1.0, 0.9],\n",
    "    \"weight_decay\": [0.0, 1e-4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 64 parameter combinations.\n"
     ]
    }
   ],
   "source": [
    "# --- Function to Generate Parameter Combinations ---\n",
    "def generate_parameter_combinations(base_params, variations_dict):\n",
    "    \"\"\"\n",
    "    Generates a list of parameter dictionaries, representing all combinations\n",
    "    of the variations provided.\n",
    "\n",
    "    Args:\n",
    "        base_params (dict): Dictionary of default parameters.\n",
    "        variations_dict (dict): Dictionary where keys are parameter names and\n",
    "                                values are lists of possible settings.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each representing a unique experiment configuration.\n",
    "    \"\"\"\n",
    "    keys = list(variations_dict.keys())\n",
    "    value_lists = [variations_dict[key] for key in keys]\n",
    "\n",
    "    all_combinations = []\n",
    "    # Use itertools.product to efficiently get the Cartesian product of all value lists\n",
    "    for value_combination in itertools.product(*value_lists):\n",
    "        # Start with a fresh copy of the base parameters for each combination\n",
    "        params = copy.deepcopy(base_params)\n",
    "        # Create a dictionary for the current combination of varying parameters\n",
    "        variation_params = dict(zip(keys, value_combination))\n",
    "        # Update the base parameters with the current variations\n",
    "        params.update(variation_params)\n",
    "        all_combinations.append(params)\n",
    "\n",
    "    print(f\"Generated {len(all_combinations)} parameter combinations.\")\n",
    "    return all_combinations\n",
    "\n",
    "# --- Generate all parameter sets ---\n",
    "all_params_list = generate_parameter_combinations(base_params, parameter_variations)\n",
    "total_experiments = len(all_params_list)\n",
    "\n",
    "# --- Define Keys to Exclude from Command Line Arguments ---\n",
    "# These keys are handled internally or set via model overrides, not passed directly via command line flags\n",
    "# derived from the base combination dictionary in the same way for all models.\n",
    "exclude_argv_common = [\"dataset_path\", \"dataset_class\", \"model_class\", \"split_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run MedSAM Experiments ---\n",
    "print(f\"\\n--- Starting MedSAM Experiments ({total_experiments} runs) ---\")\n",
    "medsam_script = \"./medsam_train_torchtrainer.py\"\n",
    "medsam_overrides = {\n",
    "    \"resize_size\": \"1024 1024\",\n",
    "    \"model_class\": \"medsam\",\n",
    "    \"experiment_name\": \"medsam_runs\"\n",
    "}\n",
    "\n",
    "for i, base_combo_params in enumerate(all_params_list):\n",
    "    params = copy.deepcopy(base_combo_params)\n",
    "\n",
    "    # Apply MedSAM specific overrides\n",
    "    params.update(medsam_overrides)\n",
    "\n",
    "    # Generate the split_strategy string dynamically using the 'split_size'\n",
    "    split_size = params['split_strategy']\n",
    "    params['split_strategy'] = ','.join(random.sample(names, split_size))\n",
    "\n",
    "    # Construct a unique run_name reflecting the parameters for this MedSAM run\n",
    "    # Note: Original MedSAM run_name didn't include split strategy details like count, using split_size instead.\n",
    "    params[\"run_name\"] = f\"medsam_{split_size}_{params['resize_size'].replace(' ','x')}_{params['loss_function']}_{params['num_epochs']}_{params['lr']}_{params['bs_train']}_{params['bs_valid']}_{params['weight_decay']}\"\n",
    "\n",
    "    print(f\"\\nRunning MedSAM experiment {i+1}/{total_experiments}: {params['run_name']}\")\n",
    "    # print(\"Parameters:\", params) # Uncomment for detailed parameter debugging\n",
    "\n",
    "    print(\"Running MedSAM experiment with:\")\n",
    "    commandline = ' '.join(dict_to_argv(params, [\"dataset_path\", \"dataset_class\", \"model_class\"]))\n",
    "    print(f\"python {medsam_script} {commandline}\")\n",
    "    !python {medsam_script} {commandline}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run U-Mamba Experiments ---\n",
    "print(f\"\\n--- Starting U-Mamba Experiments ({total_experiments} runs) ---\")\n",
    "umamba_script = \"./umamba_train_torchtrainer.py\"\n",
    "umamba_overrides = {\n",
    "    \"resize_size\": \"256 256\",\n",
    "    \"model_class\": \"umamba\",\n",
    "    \"experiment_name\": \"umamba_runs\"\n",
    "}\n",
    "umamba_exclude_argv = exclude_argv_common # U-Mamba uses val_img_indices\n",
    "\n",
    "for i, base_combo_params in enumerate(all_params_list):\n",
    "    params = copy.deepcopy(base_combo_params)\n",
    "    params.update(umamba_overrides)\n",
    "\n",
    "    split_size = params['split_strategy']\n",
    "    params['split_strategy'] = ','.join(random.sample(names, split_size))\n",
    "\n",
    "    # Construct U-Mamba run_name, potentially including val_img_indices\n",
    "    val_indices_str = params['val_img_indices'].replace(' ','') # Format val indices for filename\n",
    "    params[\"run_name\"] = f\"umamba_{split_size}_{params['resize_size'].replace(' ','x')}_{params['loss_function']}_{params['num_epochs']}_{params['lr']}_{params['bs_train']}_{params['bs_valid']}_{params['weight_decay']}_val{val_indices_str}\"\n",
    "\n",
    "    print(f\"\\nRunning U-Mamba experiment {i+1}/{total_experiments}: {params['run_name']}\")\n",
    "    # print(\"Parameters:\", params) # Uncomment for detailed parameter debugging\n",
    "\n",
    "    print(\"Running U-Mamba experiment with:\")\n",
    "    commandline = ' '.join(dict_to_argv(params, [\"dataset_path\", \"dataset_class\", \"model_class\"]))\n",
    "    print(f\"python {umamba_script} {commandline}\")\n",
    "    !python {umamba_script} {commandline}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting U-Net Experiments (64 runs) ---\n",
      "\n",
      "Running U-Net experiment 1/64: unet_20_256x256_bce_1000_0.001_2_2_0.0_val0123\n",
      "Running U-Net experiment with:\n",
      "python unet_train_torchtrainer.py /home/fonta42/Desktop/masters-degree/data/torch-trainer/VessMAP vessmap_few unet_smp --experiments_path /home/fonta42/Desktop/masters-degree/experiments/torch-trainer --run_name unet_20_256x256_bce_1000_0.001_2_2_0.0_val0123 --validate_every 50 --copy_model_every 0 --wandb_project uncategorized --resize_size 256 256 --loss_function bce --num_epochs 1000 --validation_metric Dice --lr 0.001 --lr_decay 1.0 --bs_train 2 --bs_valid 2 --weight_decay 0.0 --optimizer adam --momentum 0.9 --seed 42 --device cuda:0 --num_workers 5 --benchmark --split_strategy 4404,9284,4938,12455,5740,5801,16707,11411,8353,9860,17035,10571,13528,2849,8429,1816,11161,2546,7413,13114 --val_img_indices 0 1 2 3 --experiment_name unet_runs\n",
      "Setting up the experiment...\n",
      "Done setting up.\n",
      "Training has started\n",
      "Epochs:   0%|\u001b[34m                                      \u001b[0m| 0/1000 [00:00<?, ?epochs/s]\u001b[0m\n",
      "Training:   0%|\u001b[34m                                      \u001b[0m| 0/10 [00:00<?, ?batchs/s]\u001b[0m\u001b[A\n",
      "Training:  10%|\u001b[34m███                           \u001b[0m| 1/10 [00:01<00:10,  1.11s/batchs]\u001b[0m\u001b[A\n",
      "Training:  50%|\u001b[34m███████████████               \u001b[0m| 5/10 [00:01<00:00,  5.13batchs/s]\u001b[0m\u001b[A\n",
      "Training:  80%|\u001b[34m████████████████████████      \u001b[0m| 8/10 [00:01<00:00,  8.24batchs/s]\u001b[0m\u001b[A\n",
      "                                                                                \u001b[A\n",
      "Validating:   0%|\u001b[32m                                    \u001b[0m| 0/40 [00:00<?, ?batchs/s]\u001b[0m\u001b[A\n",
      "Validating:  20%|\u001b[32m█████▌                      \u001b[0m| 8/40 [00:00<00:00, 74.51batchs/s]\u001b[0m\u001b[A\n",
      "Validating:  45%|\u001b[32m████████████▏              \u001b[0m| 18/40 [00:00<00:00, 85.32batchs/s]\u001b[0m\u001b[A\n",
      "Validating:  70%|\u001b[32m██████████████████▉        \u001b[0m| 28/40 [00:00<00:00, 88.68batchs/s]\u001b[0m\u001b[A\n",
      "Validating:  95%|\u001b[32m█████████████████████████▋ \u001b[0m| 38/40 [00:00<00:00, 91.30batchs/s]\u001b[0m\u001b[A\n",
      "Epochs:   0%|\u001b[34m \u001b[0m| 0/1000 [00:02<?, ?epochs/s, Train loss=0.831, Validation loss=34\u001b[0m\n",
      "Training has finished\n",
      "^C\n",
      "Exception ignored in atexit callback: <bound method finalize._exitfunc of <class 'weakref.finalize'>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/weakref.py\", line 666, in _exitfunc\n",
      "    f()\n",
      "KeyboardInterrupt: \n",
      "\n",
      "Running U-Net experiment 2/64: unet_20_256x256_bce_1000_0.001_2_2_0.0001_val0123\n",
      "Running U-Net experiment with:\n",
      "python unet_train_torchtrainer.py /home/fonta42/Desktop/masters-degree/data/torch-trainer/VessMAP vessmap_few unet_smp --experiments_path /home/fonta42/Desktop/masters-degree/experiments/torch-trainer --run_name unet_20_256x256_bce_1000_0.001_2_2_0.0001_val0123 --validate_every 50 --copy_model_every 0 --wandb_project uncategorized --resize_size 256 256 --loss_function bce --num_epochs 1000 --validation_metric Dice --lr 0.001 --lr_decay 1.0 --bs_train 2 --bs_valid 2 --weight_decay 0.0001 --optimizer adam --momentum 0.9 --seed 42 --device cuda:0 --num_workers 5 --benchmark --split_strategy 8284,2413,4938,9452,14121,11828,16766,18035,18180,9284,7865,10084,13128,6384,14690,525,3406,6887,4404,4739 --val_img_indices 0 1 2 3 --experiment_name unet_runs\n",
      "Setting up the experiment...\n",
      "Done setting up.\n",
      "Training has started\n",
      "Epochs:   0%|\u001b[34m                                      \u001b[0m| 0/1000 [00:00<?, ?epochs/s]\u001b[0m\n",
      "Training:   0%|\u001b[34m                                      \u001b[0m| 0/10 [00:00<?, ?batchs/s]\u001b[0m\u001b[A\n",
      "Training:  10%|\u001b[34m███                           \u001b[0m| 1/10 [00:01<00:10,  1.13s/batchs]\u001b[0m\u001b[A\n",
      "Training:  40%|\u001b[34m████████████                  \u001b[0m| 4/10 [00:01<00:01,  4.13batchs/s]\u001b[0m\u001b[A\n",
      "Training:  70%|\u001b[34m█████████████████████         \u001b[0m| 7/10 [00:01<00:00,  7.45batchs/s]\u001b[0m\u001b[A\n",
      "Training: 100%|\u001b[34m█████████████████████████████\u001b[0m| 10/10 [00:01<00:00, 10.70batchs/s]\u001b[0m\u001b[A\n",
      "                                                                                \u001b[A\n",
      "Validating:   0%|\u001b[32m                                    \u001b[0m| 0/40 [00:00<?, ?batchs/s]\u001b[0m\u001b[A\n",
      "Validating:  20%|\u001b[32m█████▌                      \u001b[0m| 8/40 [00:00<00:00, 75.86batchs/s]\u001b[0m\u001b[A\n",
      "Validating:  45%|\u001b[32m████████████▏              \u001b[0m| 18/40 [00:00<00:00, 86.70batchs/s]\u001b[0m\u001b[A\n",
      "Validating:  70%|\u001b[32m██████████████████▉        \u001b[0m| 28/40 [00:00<00:00, 89.90batchs/s]\u001b[0m\u001b[A\n",
      "Validating:  95%|\u001b[32m█████████████████████████▋ \u001b[0m| 38/40 [00:00<00:00, 90.92batchs/s]\u001b[0m\u001b[A\n",
      "Epochs:   0%|\u001b[34m \u001b[0m| 1/1000 [00:03<55:26,  3.33s/epochs, Train loss=0.757, Validation\u001b[0m\n",
      "Training:   0%|\u001b[34m                                      \u001b[0m| 0/10 [00:00<?, ?batchs/s]\u001b[0m\u001b[A\n",
      "Training:  30%|\u001b[34m█████████                     \u001b[0m| 3/10 [00:00<00:00, 24.23batchs/s]\u001b[0m\u001b[A\n",
      "Training:  60%|\u001b[34m██████████████████            \u001b[0m| 6/10 [00:00<00:00, 24.41batchs/s]\u001b[0m\u001b[A\n",
      "Training:  90%|\u001b[34m███████████████████████████   \u001b[0m| 9/10 [00:00<00:00, 24.59batchs/s]\u001b[0m\u001b[A\n",
      "Epochs:   0%|\u001b[34m            \u001b[0m| 2/1000 [00:04<36:01,  2.17s/epochs, Train loss=0.517]\u001b[0m\n",
      "Training:   0%|\u001b[34m                                      \u001b[0m| 0/10 [00:00<?, ?batchs/s]\u001b[0m\u001b[A\n",
      "Training:  30%|\u001b[34m█████████                     \u001b[0m| 3/10 [00:00<00:00, 25.10batchs/s]\u001b[0m\u001b[A\n",
      "Training:  60%|\u001b[34m██████████████████            \u001b[0m| 6/10 [00:00<00:00, 24.81batchs/s]\u001b[0m\u001b[A\n",
      "Training:  90%|\u001b[34m███████████████████████████   \u001b[0m| 9/10 [00:00<00:00, 24.68batchs/s]\u001b[0m\u001b[A\n",
      "Epochs:   0%|\u001b[34m            \u001b[0m| 3/1000 [00:05<28:02,  1.69s/epochs, Train loss=0.414]\u001b[0m\n",
      "Training:   0%|\u001b[34m                                      \u001b[0m| 0/10 [00:00<?, ?batchs/s]\u001b[0m\u001b[A\n",
      "Training:  30%|\u001b[34m█████████                     \u001b[0m| 3/10 [00:00<00:00, 25.04batchs/s]\u001b[0m\u001b[A\n",
      "Training:  60%|\u001b[34m██████████████████            \u001b[0m| 6/10 [00:00<00:00, 24.78batchs/s]\u001b[0m\u001b[A\n",
      "Training:  90%|\u001b[34m███████████████████████████   \u001b[0m| 9/10 [00:00<00:00, 24.77batchs/s]\u001b[0m\u001b[A\n",
      "Epochs:   0%|\u001b[34m            \u001b[0m| 3/1000 [00:06<28:02,  1.69s/epochs, Train loss=0.352]\u001b[0m"
     ]
    }
   ],
   "source": [
    "# --- Run U-Net Experiments ---\n",
    "print(f\"\\n--- Starting U-Net Experiments ({total_experiments} runs) ---\")\n",
    "unet_script = \"unet_train_torchtrainer.py\" # Assuming this is the correct script name\n",
    "unet_overrides = {\n",
    "    \"resize_size\": \"256 256\",\n",
    "    \"model_class\": \"unet_smp\", # As used in the original code\n",
    "    \"experiment_name\": \"unet_runs\"\n",
    "}\n",
    "unet_exclude_argv = exclude_argv_common # U-Net uses val_img_indices\n",
    "\n",
    "for i, base_combo_params in enumerate(all_params_list):\n",
    "    params = copy.deepcopy(base_combo_params)\n",
    "    params.update(unet_overrides)\n",
    "\n",
    "    split_size = params['split_strategy']\n",
    "    params['split_strategy'] = ','.join(random.sample(names, split_size))\n",
    "\n",
    "    # Construct U-Net run_name\n",
    "    val_indices_str = params['val_img_indices'].replace(' ','')\n",
    "    params[\"run_name\"] = f\"unet_{split_size}_{params['resize_size'].replace(' ','x')}_{params['loss_function']}_{params['num_epochs']}_{params['lr']}_{params['bs_train']}_{params['bs_valid']}_{params['weight_decay']}_val{val_indices_str}\"\n",
    "\n",
    "    print(f\"\\nRunning U-Net experiment {i+1}/{total_experiments}: {params['run_name']}\")\n",
    "    # print(\"Parameters:\", params) # Uncomment for detailed parameter debugging\n",
    "\n",
    "    print(\"Running U-Net experiment with:\")\n",
    "    commandline = ' '.join(dict_to_argv(params, [\"dataset_path\", \"dataset_class\", \"model_class\"]))\n",
    "    print(f\"python unet_train_torchtrainer.py {commandline}\")\n",
    "    !python {unet_script} {commandline}\n",
    "\n",
    "\n",
    "print(\"\\n--- All experiments finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtrainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
