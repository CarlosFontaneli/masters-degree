Got it! I will conduct an in-depth research into the theoretical concepts underlying the practical implementation of the U-Mamba model, focusing mainly on its official repository while incorporating other relevant sources where available.

I will explore U-Mamba’s architecture, training methodology, loss functions, dataset processing, and its integration into vessel segmentation tasks. I will also analyze the design choices behind its core components, hyperparameter tuning, and best practices for building a custom training script.

I'll get back to you with my findings soon.

# 1. Architecture and Model Design

[U-Mamba](https://u-mamba.github.io/) _Figure: **U-Mamba architecture.** (a) U-Mamba building block consisting of two residual convolutional blocks followed by a Mamba (SSM-based) block. (b) U-Mamba’s U-shaped encoder-decoder with U-Mamba blocks in the encoder and regular residual blocks in the decoder, connected by skip pathways [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=Image%3A%20Refer%20to%20caption%20,are%20presented%20in%20Table%C2%A0%2027)._

### **Encoder-Decoder Backbone:**

U-Mamba follows a U-Net style encoder-decoder design that extracts multi-scale features and then upsamples for segmentation [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=Image%3A%20Refer%20to%20caption%20,are%20presented%20in%20Table%C2%A0%2027). The encoder is **deep**, composed of multiple stages of down-sampling, while the decoder mirrors it with up-sampling. Crucially, U-Mamba replaces standard convolutional blocks in the encoder with its hybrid **U-Mamba blocks** to capture both local and global context [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=Image%3A%20Refer%20to%20caption%20,are%20presented%20in%20Table%C2%A0%2027) [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=Fig.%C2%A01b%20shows%20the%20complete%20U,and%20the%20others%20are%20plain). The decoder uses conventional residual convolution blocks to focus on reconstructing fine details, and skip connections link encoder and decoder feature maps at each resolution, preserving spatial details [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=Image%3A%20Refer%20to%20caption%20,are%20presented%20in%20Table%C2%A0%2027) [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=Fig.%C2%A01b%20shows%20the%20complete%20U,and%20the%20others%20are%20plain).

### **Mamba Block (SSM Layer):**

The core novelty is the **Mamba block**, a module based on Selective State-Space Models (SSMs) designed for long-range dependency modeling [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=Image%3A%20Refer%20to%20caption%20,are%20presented%20in%20Table%C2%A0%2027) [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=U,a%201D%20convolutional%20layer%2C%20SiLU). Inside a U-Mamba block, after two sequential residual conv blocks, features are _flattened_ (spatial dimensions flattened into a sequence) and fed into the Mamba block [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=U,a%201D%20convolutional%20layer%2C%20SiLU) [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=After%20passing%20the%20Layer%20Normalization%C2%A0,reshaped%20and%20transposed%20to). The Mamba block has two parallel branches operating on the sequence (length _L_ = number of pixels, _C_ = channels): one branch applies a learnable 1D convolution, a SiLU nonlinearity, and an SSM layer (the Mamba SSM) to transform the sequence, while the other branch applies a linear layer and SiLU activation [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=After%20passing%20the%20Layer%20Normalization%C2%A0,reshaped%20and%20transposed%20to). The two transformed sequences are multiplied element-wise (Hadamard product) as a form of gating or feature selection [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=After%20passing%20the%20Layer%20Normalization%C2%A0,reshaped%20and%20transposed%20to). Finally, a linear projection compresses the sequence back to the original channel dimensionality and the data is reshaped back to the spatial grid [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=After%20passing%20the%20Layer%20Normalization%C2%A0,reshaped%20and%20transposed%20to). This design lets the network **filter and mix information globally** across the image: the SSM branch learns long-range interactions (modeling very long sequences efficiently), and the second branch provides an input-dependent gate, so only relevant global context is merged into the features [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=After%20passing%20the%20Layer%20Normalization%C2%A0,reshaped%20and%20transposed%20to). In essence, the Mamba layer enables **linear-time sequence processing** of the flattened image, capturing continuity of elongated structures that span large image regions [Serp-Mamba: Advancing High-Resolution Retinal Vessel Segmentation with Selective State-Space Model](https://arxiv.org/html/2409.04356v1#:~:text=segmentation%20of%20vessels%20in%20UWF,observations%2C%20we%20first%20devise%20a).

### **Residual Convolutional Blocks:**

Preceding each Mamba layer are two Residual blocks that use standard convolutional operations to extract local features [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=U,a%201D%20convolutional%20layer%2C%20SiLU). Each Residual block in U-Mamba consists of a convolution followed by Instance Normalization and a Leaky ReLU activation [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=U,a%201D%20convolutional%20layer%2C%20SiLU). These blocks focus on local texture/detail extraction and feed into the Mamba block, which then injects global context. The residual connection in these blocks helps stabilize training of the deep network. By stacking two conv blocks before the SSM, U-Mamba ensures that **local feature extraction** (edges, small structures) is done in the convolutional path, after which the Mamba unit can **aggregate long-range information** on those features [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=U,a%201D%20convolutional%20layer%2C%20SiLU). This hybrid design (Conv + SSM) leverages the strengths of CNNs for locality and SSMs for long-range dependency [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=Image%3A%20Refer%20to%20caption%20,are%20presented%20in%20Table%C2%A0%2027) [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=U,a%201D%20convolutional%20layer%2C%20SiLU).

### [^1] **Deep Supervision Mechanism:**

[^1]: **Note:** Talk about and check if we should keep this enable for comparison training script

Like nnU-Net, U-Mamba supports **deep supervision** during training. When enabled, intermediate outputs are taken from decoder layers at multiple resolutions and compared to downsampled ground truth masks, typically with gradually lower weighting for deeper outputs. This deep supervision provides additional guidance to early decoder stages, improving gradient flow and helping the model learn at multiple scales. In practice, the official U-Mamba implementation allows toggling deep supervision (e.g., via `enable_deep_supervision` in the trainer) and by default trains with it enabled [U-Mamba/umamba/nnunetv2/training/nnUNetTrainer/nnUNetTrainerUMambaEncNoAMP.py at main · bowang-lab/U-Mamba · GitHub](https://github.com/bowang-lab/U-Mamba/blob/main/umamba/nnunetv2/training/nnUNetTrainer/nnUNetTrainerUMambaEncNoAMP.py#:~:text=def%20build_network_architecture) [U-Mamba/umamba/nnunetv2/training/nnUNetTrainer/nnUNetTrainerUMambaEncNoAMP.py at main · bowang-lab/U-Mamba · GitHub](https://github.com/bowang-lab/U-Mamba/blob/main/umamba/nnunetv2/training/nnUNetTrainer/nnUNetTrainerUMambaEncNoAMP.py#:~:text=model%20%3D%20get_umamba_enc_2d_from_plans). Deep supervision encourages the network to produce reasonable segmentations at lower resolutions as well, which is beneficial for segmenting thin vessels that might otherwise vanish in coarse feature maps. It acts as a regularizer and is especially useful in deep architectures like U-Mamba to mitigate vanishing gradients.

### [^2] **Network Configuration "Plans" (Stages & Channels):(\*)**

[^2]: **Note:** Talk about Plans and if we should implement something similar or pre-processed with nnUnet and get the best plans for vess map or just a regular training.

U-Mamba’s network depth and width are not fixed but **dynamically determined** based on the dataset, inheriting nnU-Net’s self-configuring mechanism [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=in%20the%20endocer%2C%20Residual%20blocks,configurations%20are%20presented%20in%20Table%C2%A02) [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=comparison%20of%20U,also%20optimized%20with%20stochastic%20gradient). The number of encoder/decoder **stages** (levels in the U-Net) and the convolutional kernel sizes and pooling strides per stage are chosen according to the image size and voxel spacing of the dataset [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=comparison%20of%20U,also%20optimized%20with%20stochastic%20gradient) [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=Table%202%3A%20U,7%2C%207). For example, for a 2D endoscopy dataset, U-Mamba used 7 stages with a pooling operation 6 times in each spatial dimension (effectively 6 downsamplings) [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=Table%202%3A%20U,7%2C%207), whereas a larger microscopy image dataset was handled with 8 stages and 7 downsamplings per axis [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=Abdomen%20CT%20,7%2C%207). The **feature channel counts** at each stage typically follow nnU-Net’s heuristic (doubling with depth): e.g., if the first encoder layer has 32 channels, subsequent layers might have 64, 128, etc., though the exact base number is chosen by nnU-Net’s planning. Convolution kernel sizes are usually $3\times3$ (or $3\times3\times3$ in 3D) for most layers [The Plans_handler.py does not match the original repo. · Issue #62 · bowang-lab/U-Mamba · GitHub](https://github.com/bowang-lab/U-Mamba/issues/62#:~:text=def%20conv_kernel_sizes%28self%29%20,conv_kernel_sizes), except possibly at the highest resolution where larger kernels might be used as per nnU-Net defaults. Strided convolutions (or pooling) are used for downsampling, and transposed convolutions for upsampling in the decoder [U-Mamba](https://u-mamba.github.io/#:~:text=Overview%20of%20the%20U,decoder%2C%20together%20with%20skip%20connections). All these parameters – number of filters per layer, number of pooling/upsample layers, kernel sizes – are encapsulated in a **“plans”** file generated by nnU-Net’s preprocessing for a given dataset, and U-Mamba reads this to construct the model architecture [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=comparison%20of%20U,also%20optimized%20with%20stochastic%20gradient). This means U-Mamba **automatically adapts** to different input sizes and dimensions without manual reconfiguration [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=comparison%20of%20U,also%20optimized%20with%20stochastic%20gradient). The architecture is therefore _dynamic_: a small 2D dataset might result in a shallower U-Mamba, whereas a large 3D dataset yields a deeper network, all decided by the planning stage.

### **Dynamic Architecture & Helper Functions:**

Internally, U-Mamba’s code uses helper functions to assemble the network from a plan configuration. For instance, the repository provides functions like `get_umamba_enc_2d_from_plans` (and its 3D counterpart) which parse the nnU-Net plan (containing info such as patch size, recommended number of pooling, and per-layer specifications) and instantiate the `UMambaEnc` model accordingly [U-Mamba/umamba/nnunetv2/training/nnUNetTrainer/nnUNetTrainerUMambaEncNoAMP.py at main · bowang-lab/U-Mamba · GitHub](https://github.com/bowang-lab/U-Mamba/blob/main/umamba/nnunetv2/training/nnUNetTrainer/nnUNetTrainerUMambaEncNoAMP.py#:~:text=def%20build_network_architecture) [U-Mamba/umamba/nnunetv2/training/nnUNetTrainer/nnUNetTrainerUMambaEncNoAMP.py at main · bowang-lab/U-Mamba · GitHub](https://github.com/bowang-lab/U-Mamba/blob/main/umamba/nnunetv2/training/nnUNetTrainer/nnUNetTrainerUMambaEncNoAMP.py#:~:text=model%20%3D%20get_umamba_enc_2d_from_plans). These helper builders handle the creation of each encoder stage (inserting the correct number of U-Mamba blocks and residual blocks) and each decoder stage, as well as setting up skip connections. This programmatic assembly ensures consistency with nnU-Net’s rules. It also simplifies experimentation – for example, one could adjust a configuration in the plan or code (such as adding an extra encoder block or changing the SSM’s internal dimensions) and rebuild the model. The **self-configuring** nature combined with these helper functions means that U-Mamba’s architecture is flexible: out of the box it will match the nnU-Net optimum for a dataset, but advanced users can override parameters (like the base number of channels, or whether every encoder block or only the bottleneck uses the Mamba layer) by modifying the plan or calling the `UMambaEnc` class with custom arguments. In summary, U-Mamba’s design marries a **principled architecture (hybrid CNN-SSM blocks in a U-Net)** with a **dynamic instantiation mechanism**, allowing it to automatically scale to different problems while retaining key components like the Mamba layer for global context and residual conv layers for local detail.

# 2. Training Methodology and Loss Functions

### **Compound Loss (Dice + BCE):**

U-Mamba is trained with a **combined loss function** that is the unweighted sum of Dice loss and Binary Cross Entropy (BCE) with logits (or more generally, cross-entropy for multi-class) [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=size%2C%20and%20network%20configurations%20%28e,2D%20and%203D%20datasets%2C%20respectively) [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=descent%20and%20the%20loss%20function,more%20streamlined%20and%20efficient%20evaluation). This composite loss is widely used in segmentation, especially for imbalanced targets like vessels. The **Dice loss** component directly optimizes the overlap (Dice Similarity) between the predicted mask and ground truth, which helps the model handle the class imbalance inherent in vessel segmentation – the vessel pixels (foreground) are far fewer than background pixels [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=descent%20and%20the%20loss%20function,more%20streamlined%20and%20efficient%20evaluation). Dice loss gives more relative weight to improving overlap on the small vessel regions. The **BCEWithLogitsLoss** (binary cross-entropy on logits) component, on the other hand, treats each pixel independently and provides a strong signal for correctly classifying each pixel. Using both in tandem has proven robust across many medical segmentation tasks [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=descent%20and%20the%20loss%20function,more%20streamlined%20and%20efficient%20evaluation). In vessel segmentation, this combination is particularly effective: Dice loss alone can sometimes be unstable when vessels occupy a tiny fraction of the image (it can yield high gradients only when there is some overlap), while BCE alone tends to be overwhelmed by easy negative (background) pixels. The Dice+BCE compound loss encourages the network to **maximize overall segmentation accuracy while also focusing on overlap/coverage of the vessel structures**, which yields better detection of thin vessels [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=descent%20and%20the%20loss%20function,more%20streamlined%20and%20efficient%20evaluation). Empirically, this unweighted sum has been found to balance precision and recall for vessels without needing manual re-weighting [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=descent%20and%20the%20loss%20function,more%20streamlined%20and%20efficient%20evaluation). (In multi-class scenarios, one could use Dice + cross-entropy per class; for a single vessel class vs background, Dice + BCE is equivalent.) This approach is consistent with nnU-Net’s default training, as U-Mamba was trained under the same setting [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=size%2C%20and%20network%20configurations%20%28e,2D%20and%203D%20datasets%2C%20respectively). It is worth noting that other loss function tweaks have been explored for vessels – e.g. some works incorporate a **topology-preserving loss** – but the baseline Dice+BCE remains a strong choice for general use.

### [^3] **Optimizer Choice:**

For optimization, the official U-Mamba training used **Stochastic Gradient Descent (SGD)** with momentum [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=size%2C%20and%20network%20configurations%20%28e,2D%20and%203D%20datasets%2C%20respectively), following the nnU-Net framework. SGD was likely chosen for its stability on large segmentation networks and because nnU-Net’s default uses SGD with a learning rate schedule. The training ran for a relatively large number of epochs (e.g. 1000 epochs in their experiments) to ensure convergence [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=SwinUNETR%20into%20the%20nnU,Table%C2%A0%2027). In practice, both SGD and Adam-type optimizers have been successfully used for segmentation; **nnU-Net favored SGD (with Nesterov momentum 0.99) combined with a **poly learning rate schedule** (where the learning rate decays as $(1 - \tfrac{epoch}{max\_epochs})^{0.9}$) to steadily decrease the learning rate over the training** [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=SwinUNETR%20into%20the%20nnU,Table%C2%A0%2027). U-Mamba inherits this regime: an initial learning rate (e.g. 0.01 for SGD) gradually decays, which helps avoid overshooting and ensures fine convergence at the end of training. Other optimizers could be used as well – indeed, in benchmarking, the authors trained comparison models with their recommended optimizers (Adam or AdamW for Transfomer-based models) [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=We%20compared%20U,Table%C2%A0%2027). If one trains U-Mamba outside nnU-Net, **AdamW** is a good alternative (often with a slightly lower initial lr, e.g. 1e-4) as it can converge faster on segmentation, but careful scheduling or a validation-based LR reduction is recommended to prevent overfitting thin structures. In either case, using a **learning rate scheduler** (poly decay, step decay, or cosine annealing) is important for segmentation tasks to fine-tune the model in later epochs.

[^3]: **Note:** Should we also implement this weight decay on the other training for models?

### **Gradient Clipping:**

Training U-Mamba involves learning both convolution weights and the state-space parameters of the Mamba blocks. The Mamba (SSM) layer introduces recurrent-style long-sequence processing which can sometimes lead to numerical instabilities. In the official implementation, one challenge observed was that using Automatic Mixed Precision (AMP) caused gradient instabilities – training loss would become NaN – likely due to precision issues in the SSM calculations [Nan When training · Issue #8 · bowang-lab/U-Mamba · GitHub](https://github.com/bowang-lab/U-Mamba/issues/8#:~:text=Thanks%20for%20your%20valuable%20feedback,AMP%20will%20lead%20to%20nan). The developers resolved this by disabling AMP and providing a trainer variant that runs in full precision [Nan When training · Issue #8 · bowang-lab/U-Mamba · GitHub](https://github.com/bowang-lab/U-Mamba/issues/8#:~:text=Thanks%20for%20your%20valuable%20feedback,AMP%20will%20lead%20to%20nan). While they did not explicitly mention using gradient clipping, it is a known best practice in such contexts. **Gradient clipping** (e.g. clipping norm or value) can prevent exploding gradients that might arise from the recurrent nature of SSM or from the dice loss when encountering very small segmentation regions. In vessel segmentation, clipping the gradient norm (to, say, 5 or 10) can help stabilize training when tiny structures cause large loss spikes. It hasn’t been reported that U-Mamba needed aggressive clipping, but if one experiences divergence or NaNs, applying a gradient clip is a prudent measure. In summary, **ensuring numerical stability** is key: if training with mixed precision, monitor for NaNs (and consider full precision if they occur [Nan When training · Issue #8 · bowang-lab/U-Mamba · GitHub](https://github.com/bowang-lab/U-Mamba/issues/8#:~:text=Thanks%20for%20your%20valuable%20feedback,AMP%20will%20lead%20to%20nan)), and use gradient clipping or a lower learning rate if gradients blow up. These techniques help U-Mamba train reliably even as it models long-range sequences.

### **Learning Rate and Schedule:**

As noted, U-Mamba’s training likely used nnU-Net’s default schedule, which is a **poly schedule** over a fixed epoch count [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=SwinUNETR%20into%20the%20nnU,Table%C2%A0%2027). This schedule starts with a base learning rate (e.g. 0.01 for SGD) and reduces it smoothly each epoch without oscillation. An alternative is **cyclical schedules** or **warm restarts**, but those are less common in segmentation. The long training (e.g. 1000 epochs) with decaying LR ensures the model has time to refine the segmentation of very small vessels, which might only get properly learned once the larger structures are already settled. For vessel segmentation specifically, one might also use an **early stopping** criterion on validation Dice if overfitting is a concern, since extremely long training could start eroding performance on thin vessels (if the model begins to favor only larger vessels). However, nnU-Net found that a fixed long schedule with decay produces excellent results, likely because the decaying LR prevents overfitting late in training.

### **Evaluation Metrics for Vessel Segmentation:**

Choosing the right evaluation metrics is crucial to gauge performance on vessel segmentation, as simple accuracy can be misleading due to class imbalance. The **Dice Similarity Coefficient (DSC)**, equivalent to the F1-score for binary segmentation, is a primary metric – it directly measures overlap and is sensitive to both false negatives (missing vessels) and false positives [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=Following%20the%20recommendations%20in%20Metrics,is%20an%20instance%20segmentation%20task). Dice (or the closely related IoU) is effective for overall segmentation quality and is commonly reported in vessel segmentation challenges. However, to capture the specific challenges of vessels, additional metrics are used. **Sensitivity (recall)** and **specificity** are often reported for retinal vessel segmentation (e.g. in the DRIVE dataset) to measure how well the model detects vessels (recall) versus how well it avoids false alarms (specificity). A high sensitivity is important to ensure even thin vessels are detected. Many works also compute the **ROC curve and AUC**, treating vessel segmentation as a pixel-wise classification problem [Comparison between BCE Loss Function, Dice ... - ResearchGate](https://www.researchgate.net/figure/Comparison-between-BCE-Loss-Function-Dice-Loss-Function-and-DBCE-Loss-Function-for_fig6_371441145#:~:text=Comparison%20between%20BCE%20Loss%20Function%2C,fields%2C%20including%20retinal%20vessel). The AUC summarizes performance across thresholds and is useful when comparing models that might favor different trade-offs.

Notably, specialized metrics have been introduced for vessel topology. The **centerline Dice (clDice)** metric compares the skeleton (centerline) of the predicted vessels to the skeleton of the ground truth, as well as the full masks, ensuring that connectivity of vessels is preserved [Centerline Boundary Dice Loss for Vascular Segmentation | MICCAI 2024 - Open Access](https://papers.miccai.org/miccai-2024/129-Paper0458.html#:~:text=Vascular%20segmentation%20in%20medical%20imaging,aware). A high clDice means the model got the thin structure nearly connected even if widths differ. This is extremely pertinent for long, thin anatomy: a prediction that breaks a vessel into pieces would have a lower clDice. Some methods optimize clDice as a loss to encourage topologically correct segmentation. There is also the recently proposed **centerline-boundary Dice (cbDice)** which extends clDice by also considering vessel thickness consistency [Centerline Boundary Dice Loss for Vascular Segmentation | MICCAI 2024 - Open Access](https://papers.miccai.org/miccai-2024/129-Paper0458.html#:~:text=centerline%20Dice%20,DoU%29%20loss%20through%20a%20mask). These metrics address the fact that a model could score a good standard Dice by capturing large vessels but still miss many fine branches – whereas clDice will penalize missing those branches’ centerlines [Centerline Boundary Dice Loss for Vascular Segmentation | MICCAI 2024 - Open Access](https://papers.miccai.org/miccai-2024/129-Paper0458.html#:~:text=Vascular%20segmentation%20in%20medical%20imaging,aware). For practical evaluation, one should use a combination: **Dice/F1 for overall agreement**, and optionally **clDice for connectivity**. In cases where a method explicitly targets preserving connectivity, clDice is a very insightful metric. Additionally, **Normalized Surface Distance (NSD)** or Hausdorff distance can be considered if the exact boundary precision is important (nnU-Net authors, for instance, reported NSD for organ segmentation [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=Following%20the%20recommendations%20in%20Metrics,is%20an%20instance%20segmentation%20task), though for thin vessels surface distance is less meaningful than for larger structures). In summary, for vessel segmentation tasks the most effective metrics are those that capture both area overlap and the integrity of thin structures: **Dice/F1** (or IoU) for overlap, **Precision/Recall** or **AUC** for pixel classification performance, and **clDice** for connectivity/topology of the vessel network [Centerline Boundary Dice Loss for Vascular Segmentation | MICCAI 2024 - Open Access](https://papers.miccai.org/miccai-2024/129-Paper0458.html#:~:text=Vascular%20segmentation%20in%20medical%20imaging,aware).

# 3. Dataset Processing and Preprocessing

### [^4] **Challenges of Vessel Segmentation Data:**

Vessel segmentation poses unique challenges compared to more blob-like or solid-organ segmentation. Vessels form **thin, elongated structures** that can span long distances across an image with very small width. This means that a segmentation model must be extremely good at capturing long-range continuity – a small break in prediction can sever a vessel. The thin structure also leads to extreme **class imbalance**: only a tiny fraction of pixels are vessel, the rest are background. As image resolution increases (for example, ultra-wide field retinal images), this imbalance grows even more severe [Serp-Mamba: Advancing High-Resolution Retinal Vessel Segmentation with Selective State-Space Model](https://arxiv.org/html/2409.04356v1#:~:text=and%20delicate%20nature%20of%20the,resolution%20images.%20Our%20ADDR%20module). Additionally, vessels often exhibit a **multi-scale structure** – there are thick primary vessels and many finer branches. Capturing both large and tiny vessels in one model is challenging, since typical CNN kernels could miss the fine lines if tuned for broader structures [OCTAMamba: A State-Space Model Approach for Precision OCTA Vasculature Segmentation](https://arxiv.org/html/2409.08000v1#:~:text=Optical%20Coherence%20Tomography%20Angiography%20,scale%20vasculature%2C%20and%20a%20Focused). The imaging conditions introduce further difficulties: in modalities like fundus photography or OCT angiography, **low contrast and noise** are common. Thin vessels may have low contrast against tissue, and noise or artifacts can easily obscure them [OCTAMamba: A State-Space Model Approach for Precision OCTA Vasculature Segmentation](https://arxiv.org/html/2409.08000v1#:~:text=Optical%20Coherence%20Tomography%20Angiography%20,scale%20vasculature%2C%20and%20a%20Focused). Moreover, variations in anatomy (e.g., some patients have more visible small vessels than others) mean a model must generalize to different vessel patterns. Because vessels are essentially tree- or graph-like shapes, **topology matters**: one cares not just about pixel-wise accuracy but also that the vessel network in the prediction is connected similarly to the ground truth. All these factors – elongation, multi-scale branching, class imbalance, low contrast – make preprocessing and data augmentation particularly important to present the data to the model in the best way.

[^4]: **Note:** Talk about the paper [OCTAMamba: A State-Space Model Approach for Precision OCTA Vasculature Segmentation](https://arxiv.org/html/2409.08000v1#:~:text=Optical%20Coherence%20Tomography%20Angiography%20,scale%20vasculature%2C%20and%20a%20Focused)

**Preprocessing Techniques:** A fundamental step is **intensity normalization**. For instance, in retinal vessel segmentation (which often uses color fundus images), one might convert the image to grayscale or use the green channel (which typically has best vessel contrast) and then apply contrast enhancement. Many pipelines apply **histogram equalization or CLAHE** to make vessels more distinguishable. In U-Mamba’s case, since it leverages nnU-Net’s preprocessing, it inherits robust normalization steps (e.g. clipping extreme intensities and z-score normalization of each image) [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=SwinUNETR%20into%20the%20nnU,Table%C2%A0%2027). Ensuring all images are on a similar intensity scale helps the model not be confused by brightness differences. Another useful preprocessing specific to vessels is applying a **vesselness filter** (like the Frangi filter or other Hessian-based filters) to accentuate thin curvilinear structures. For example, a Frangi filter can take a raw image and produce an image emphasizing line-like structures; this can be used as an additional input channel or to refine the image for segmentation. As noted in one survey, such filters “enhance thin and elongated structures in images, making it well-suited for vessel segmentation” [A Comprehensive Overview of Deep Learning Techniques for ...](https://biomedgrid.com/fulltext/volume19/a-comprehensive-overview-of-deep-learning-techniques-for-retinal-vessel-segmentation.002631.php#:~:text=A%20Comprehensive%20Overview%20of%20Deep,Otsu%27s%20thresholding%20and%20tensor). In some cases, especially with 3D medical images (like CT angiography), one might segment a smaller region of interest: for instance, if the vessels occupy only a certain part, cropping the volume to that region reduces irrelevant background. **Resampling** is also important – if images have anisotropic spacing (e.g., slices in a 3D angiogram are thicker), nnU-Net’s preprocessing will resample to a more uniform resolution so that vessels have similar scale in each dimension. This prevents, say, a vessel appearing much thinner in one axis which could be lost. In summary, preprocessing focuses on **standardizing the data** (intensity and geometry) and possibly **enhancing vessel signal**. U-Mamba’s built-in pipeline relies on nnU-Net, which automates many of these steps (resampling, normalizing, possibly cropping out blank space) [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=SwinUNETR%20into%20the%20nnU,Table%C2%A0%2027). For custom vessel datasets, it’s wise to inspect if small vessels are being preserved after preprocessing; e.g., if downsampling an image, ensure resolution is still high enough for the smallest vessel. If not, one might need to increase patch size or choose a finer resampling.

### **Data Augmentation Strategies:**

Augmentation for vessel datasets must walk a line between diversity and structure preservation. Geometric transformations that are commonly used include **rotations** and **flips**, which are generally safe for vessels since orientation doesn’t alter appearance – vessels can be in any direction, so rotation/flip just teaches invariance. Small **scalings** or **translations** are also fine (and can simulate slightly different field of view). One must be careful with more aggressive warping: for example, heavy **elastic deformations** or random perspective changes might twist or break a continuous vessel in unrealistic ways. However, moderate non-rigid transforms can still be applied – they help the model learn to handle slight anatomical variations. A study on retinal vessels found that naive RandAugment was suboptimal and instead used a tailored set of augmentations including blur, color jitter, flips, perspective transforms, resize/crop and even CutMix [Full-scale Representation Guided Network for Retinal Vessel Segmentation](https://arxiv.org/html/2501.18921v1#:~:text=empirically%20found%20that%20RandAugment%C2%A0,25). The **key is to keep augmentations realistic** for vessel geometry. Simple flips and rotations (e.g., 90°) are definitely recommended. **Color jitter or brightness/contrast variation** is very useful in real clinical images – e.g., fundus images can be brighter or darker depending on acquisition, so randomly adjusting brightness and contrast teaches the model to rely on vessel structure, not absolute intensity. Adding a bit of **Gaussian noise** or blur can improve robustness to sensor noise or slight defocus, but these should be mild to not wash out thin vessels. If using cropping or patching, ensure the patch size is large enough to contain full lengths of vessels; if a vessel is cut at patch borders, the model might think vessels naturally end there. One strategy is to use **random crops centered on vessels** (to ensure each training patch has some vessel content and not just background). Some advanced augmentations for vessels involve creating synthetic perturbations: for example, one could randomly drop small segments of a vessel in the ground truth during training (forcing the model to learn to reconnect, though this is experimental). More commonly, techniques like **CutMix** (cut-and-paste patches between images) can be used to increase data – for vessels, this might introduce unnatural vessel discontinuities at the cut boundaries, so it should be used sparingly. The FSG-Net paper’s use of CutMix for retinal images suggests it can work if the network has enough context to mend the pasted region [Full-scale Representation Guided Network for Retinal Vessel Segmentation](https://arxiv.org/html/2501.18921v1#:~:text=empirically%20found%20that%20RandAugment%C2%A0,25). Overall, the best practice is to apply **spatial augmentations that do not distort vessel continuity too severely**: rotations, flips, slight elastic deformations, and cropping/padding are good. Intensity augmentations (jitter, noise, blur) help with generalization but should preserve the relative contrast of vessels so they remain detectable. It’s also important to **monitor augmentation effects** – for instance, too much blur could erase the thinnest vessels entirely. One can adjust the augmentation strength (e.g., limit rotation angles, limit elastic deformation grid distortions) to maintain realistic vessel shapes. By augmenting the training data, we improve the model’s robustness to variations in vessel appearance, imaging conditions, and orientation, all while making sure the underlying vessel topology isn’t compromised by the augmentation.

### [^5]**Maintaining Vessel Structure Integrity:**

A recurring theme is to maintain the **connectivity and shape** of vessels in augmented images. Practically, this means avoiding any processing that introduces artifacts like broken lines or doubled/dissimilar vessel segments. For example, if using interpolation in rotation or scaling, one should use high-quality interpolation to avoid jagged edges or gaps in thin lines. When applying random crops, one might consider reflecting or wrapping the image borders so that a vessel cut at the edge doesn’t just disappear (some implementations mirror the image at patch borders during training). Another approach is to incorporate **morphological augmentation**: for instance, one could randomly dilate or erode the vessel mask slightly (to teach the model robustness to annotation thickness), but excessive morphological changes could also confuse the model regarding vessel width. In essence, each augmentation should be tested to confirm that a human can still recognize the vessels – a good rule of thumb to maintain integrity. The literature also suggests specialized augmentation for vessels: the Serp-Mamba paper, for instance, introduces a curved scanning mechanism to feed the network vessel segments in a natural “snake-like” order to better capture continuity [Serp-Mamba: Advancing High-Resolution Retinal Vessel Segmentation with Selective State-Space Model](https://arxiv.org/html/2409.04356v1#:~:text=Space%20Model%20,transformations%2C%20ensures%20the%20effective%20and) (though this is more of an architectural change than data augmentation). The takeaway is that **preserving the continuity of thin structures is paramount** – augmentations are chosen and tuned with that in mind.

[^5]: **Note:** Talk about the paper [Serp-Mamba: Advancing High-Resolution Retinal Vessel Segmentation with Selective State-Space Model](https://arxiv.org/html/2409.04356v1#:~:text=Space%20Model%20,transformations%2C%20ensures%20the%20effective%20and)

### **Annotation Quality and Its Impact:**

Vessel segmentation accuracy is heavily influenced by the quality of annotations in the training set. Annotating thin vessels is tedious and prone to error – a slight miss can mean a whole branch is absent in ground truth. High-quality annotations should consistently mark even the small vessels; if many tiny vessels are missed by annotators, the model will simply never learn them. Likewise, if the annotation sometimes includes a tiny branch and other times not (due to ambiguity), the model may get confused or treat those pixels as uncertain. This has led researchers to emphasize consistent annotation protocols. For example, using **multi-rater consensus** or reviewing ambiguous regions can improve label quality. If annotations are thick centerlines or if they only label the vessel center versus the full width, it’s important the model output is trained to match that style – otherwise, you might evaluate incorrectly. Poor annotation can also affect loss: Dice loss on extremely thin or sporadic ground truth can produce very high gradient spikes (because a single pixel discrepancy is a large fraction of a tiny vessel). Therefore, smoothing the ground truth (e.g., slight dilation) or using soft labels can sometimes help if annotations are noisy. Moreover, annotation errors like slight misalignments (e.g., the traced vessel is a few pixels off the actual vessel in the image) can confuse the model’s learning of edge features. In critical applications, some teams even do a second pass to refine labels after seeing model outputs (iterative truth refinement). The **impact of annotation quality** is such that a model can only be as good as the ground truth it’s trained on for those fine details. It’s recommended to ensure annotations have **consistent vessel connectivity** – no unintentional breaks – and correct topology, since the model will otherwise learn those broken patterns. Additionally, one should use evaluation metrics tolerant to slight annotation differences (like clDice or boundary-aware metrics) if the annotations are a bit uncertain at very fine scales. In summary, high-quality, consistent annotations of vessels (covering the full extent of visible vessels, with minimal errors) significantly improve model performance; any gaps or mistakes in labeling thin structures will likely translate to the model missing those structures as well.

# 4. Building a Custom Training Script for U-Mamba

Implementing a custom training script for U-Mamba involves configuring the model, data pipeline, and training loop to suit your vessel segmentation task. Here’s a breakdown of important considerations:

### **Model Instantiation – `get_umamba_enc_2d_from_plans` vs. Direct `UMambaEnc`:**

The official U-Mamba repository provides convenient factory functions to create the model based on nnU-Net “plans.” Using `get_umamba_enc_2d_from_plans` is recommended if you have run the nnU-Net planning on your dataset. This function will read the automatically determined parameters (network depth, filter sizes, pooling schedule, etc.) from the plan and build a `UMambaEnc` model matching those specs [U-Mamba/umamba/nnunetv2/training/nnUNetTrainer/nnUNetTrainerUMambaEncNoAMP.py at main · bowang-lab/U-Mamba · GitHub](https://github.com/bowang-lab/U-Mamba/blob/main/umamba/nnunetv2/training/nnUNetTrainer/nnUNetTrainerUMambaEncNoAMP.py#:~:text=def%20build_network_architecture) [U-Mamba/umamba/nnunetv2/training/nnUNetTrainer/nnUNetTrainerUMambaEncNoAMP.py at main · bowang-lab/U-Mamba · GitHub](https://github.com/bowang-lab/U-Mamba/blob/main/umamba/nnunetv2/training/nnUNetTrainer/nnUNetTrainerUMambaEncNoAMP.py#:~:text=model%20%3D%20get_umamba_enc_2d_from_plans). Essentially, it ensures your U-Mamba architecture is **tailored to the dataset** (patch size, image dimensionality, and class count) with minimal manual tweaking – for example, it will create the appropriate number of encoder stages and channels as decided by nnU-Net’s analysis. On the other hand, you can instantiate the `UMambaEnc` class (for 2D or 3D) directly by providing configuration arguments (like number of stages, features per stage, etc.). Direct instantiation is useful for **experimentation or custom setups**: you might want to specify a smaller network for faster training, or force a certain number of Mamba blocks. However, doing so requires understanding the expected arguments and ensuring compatibility (e.g., the decoder must mirror the encoder). If you go this route, you’ll manually decide parameters that nnU-Net would otherwise handle – for instance, how many U-Mamba blocks (with Mamba layers) versus plain residual blocks to use, how many channels in the first layer, etc. The choice between the two approaches comes down to convenience vs. control. For most users and especially to leverage U-Mamba’s strength, using the `get_umamba_enc_from_plans` approach is easier and less error-prone. It will automatically configure details like deep supervision outputs and the final classification layer (with the correct number of classes and softmax/sigmoid) without extra work. In contrast, direct instantiation is useful if nnU-Net’s planning cannot be used (say you don’t want to run the preprocessing or you have a very custom data input), or if you want to modify the architecture beyond what the plan would do (e.g., add an extra skip connection or experiment with fewer encoder blocks). In summary, **for a custom training script it’s advisable to use `get_umamba_enc_2d_from_plans` with a properly generated plan** for your vessel dataset – this leverages the robust default configuration. If needed, you can then adjust certain parameters post-instantiation (for example, toggling `model.enable_deep_supervision = False` if you decide to disable deep supervision).

### **Default Configuration and Vessel-Specific Customizations:**

U-Mamba’s default configuration (as derived from nnU-Net) is already quite powerful for vessel segmentation, but there are a few things you might tweak for optimal results on vessels. By default, the model will output a single segmentation channel for “vessel” vs “background” (if your dataset is binary class). Ensure the final layer uses a **sigmoid** or softmax appropriately – in nnU-Net’s trainer, the output is a logit that goes into the Dice+BCE loss, so typically no explicit sigmoid in the model definition (the BCEWithLogits loss handles it). The number of feature channels per layer and number of pooling stages will default based on image size; if your retinal images are very high resolution, the default might create a deep network (possibly 6–7 downsamplings). You might consider if that’s needed – sometimes extremely deep encoders can make it hard to learn very fine details (though deep supervision mitigates that). One could customize the **number of stages** by editing the plan or overriding in code. Similarly, the **base number of filters** (the width of the first conv layer) could be increased if the dataset is large and you want more capacity, or decreased for faster inference. Another customization is deciding where to use the Mamba blocks: U-Mamba has two variants, **U-Mamba_Enc** (which uses the Mamba block in _every_ encoder stage) and **U-Mamba_Bot** (which uses a Mamba block only at the bottleneck, and uses standard residual convs in earlier encoder levels) [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=a%20convolutional%20layer%20together%20with,and%20all%20encoder%20blocks%2C%20respectively). The repository provides both variants as separate trainers (selectable via trainer name). In a custom script, you could add a flag to choose this: for U-Mamba_Bot, you would replace all but the last encoder block’s Mamba layers with simple conv blocks. The idea is that if your vessels are mostly small scale and local, using fewer SSM layers might suffice and save computation – but for extensive networks of vessels, the full U-Mamba_Enc might perform better by modeling long-range context at multiple scales. The default (Enc) is typically the stronger model for capturing long thin vessels throughout the image [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=a%20convolutional%20layer%20together%20with,and%20all%20encoder%20blocks%2C%20respectively).

Additionally, consider **deep supervision** as a hyperparameter: by default it’s on, which usually helps, but you could disable it if you find it unnecessary for a simpler training (the nnU-Net trainer has a flag `enable_deep_supervision` it passes when building the model [U-Mamba/umamba/nnunetv2/training/nnUNetTrainer/nnUNetTrainerUMambaEncNoAMP.py at main · bowang-lab/U-Mamba · GitHub](https://github.com/bowang-lab/U-Mamba/blob/main/umamba/nnunetv2/training/nnUNetTrainer/nnUNetTrainerUMambaEncNoAMP.py#:~:text=def%20build_network_architecture)). For vessel segmentation, deep supervision often _is_ helpful since it forces even coarse decoder layers to learn something about vessel structure, which can reinforce the learning of large and small vessels. So leaving it on is generally best practice (with the loss weights for deep outputs typically halved at each lower resolution, as nnU-Net does).

Finally, look at **patch size and batch size**: these are not part of the model config per se, but nnU-Net’s plan picks them. For thin vessels, using a large patch that covers a big field of view can be important (so the model can see long segments at once). The nnU-Net planner likely already chose a reasonably large patch (e.g., 512×512 for retina-like data) [U-Mamba](https://u-mamba.github.io/#:~:text=Table%202.%20U,7%2C%207). In a custom script, ensure you crop or tile images in a way that preserves vessel continuity (overlap tiles if necessary). In summary, the default U-Mamba config is a strong starting point; typical customizations involve adjusting **model capacity or depth** and choosing the **Mamba layer placement** to suit the dataset’s needs.

### **Structure of the Training Script (Logging, Checkpointing, Visualization):**

A well-designed training script for U-Mamba should include the same kind of training loop and utilities you would have for a UNet or other segmentation model. Many existing UNet training scripts (or frameworks like PyTorch Lightning) provide features you can emulate:

- **Data Loading and Augmentation:** First, set up a `Dataset`/`DataLoader` that reads your vessel images and masks, applying the preprocessing and augmentation steps discussed. Ensure your augmentation pipeline is integrated here (for example, using MONAI or Albumentations to do random flips, rotations, etc. on the fly). Use separate training and validation sets so you can monitor performance. If you followed nnU-Net’s data format, you could even use nnU-Net’s dataloader; otherwise, custom code is fine.

- **Model Initialization:** Instantiate U-Mamba (via `get_umamba_enc_2d_from_plans` or manual config as above). Move it to GPU. It’s good to print out the model architecture to verify it matches expectations [U-Mamba/umamba/nnunetv2/training/nnUNetTrainer/nnUNetTrainerUMambaEncNoAMP.py at main · bowang-lab/U-Mamba · GitHub](https://github.com/bowang-lab/U-Mamba/blob/main/umamba/nnunetv2/training/nnUNetTrainer/nnUNetTrainerUMambaEncNoAMP.py#:~:text=print%28) (the trainer code does print the model architecture summary). If deep supervision is on, note that the model’s forward will return a **tuple of outputs** (e.g., multiple prediction maps) instead of a single tensor – your training loop should handle that (summing losses from each or using nnU-Net’s built-in loss function which already does this).

- **Loss and Optimizer:** Define the Dice + BCE loss as discussed. You could implement this by using, for example, `monai.losses.DiceCELoss` or by summing `DiceLoss` and `BCEWithLogitsLoss` manually. Set up the optimizer (SGD or AdamW) with your chosen learning rate. If using PyTorch, you might also set up a LR scheduler (e.g., `torch.optim.lr_scheduler.LambdaLR` for poly decay, or ReduceLROnPlateau, etc.). Don’t forget to consider gradient clipping: you can add `torch.nn.utils.clip_grad_norm_` in the training loop if needed.

- **Training Loop:** Iterate over epochs. For each batch, get model outputs and compute loss. Because U-Mamba uses deep supervision, the output might be a list where `output[0]` is the final prediction and others are intermediate – ensure your loss function is applied properly. (In nnU-Net, their loss function knows how to handle a list by applying the same loss to each scaled target and summing.) After computing loss, backpropagate and step the optimizer. Log the training loss value. It’s helpful to print or log the loss every few iterations to monitor training progress.

- **Validation and Metrics:** Every epoch or every few epochs, evaluate on a validation set. Switch the model to eval mode, disable grad. Compute predictions for val images and measure metrics such as Dice, precision, recall, etc. This is where you’ll use your evaluation metrics for vessels – e.g., compute Dice on the vessel class, maybe sensitivity (percentage of vessel pixels correctly detected) and specificity. If the dataset is small, you can run validation each epoch; if large, maybe every 5 or 10 epochs to save time. Log these metrics. Logging can be done simply by printing or using a logging library (Python’s `logging` or even writing to a CSV). For more sophisticated setup, consider integrating **TensorBoard**: you can log scalar metrics (loss, Dice) and even images. For example, periodically log a couple of validation images with their predicted mask overlay, so you can visualize how the model is doing on thin vessels (TensorBoard or similar tools allow image logging).

- **Checkpointing:** Save model checkpoints regularly. A common practice is to save the **best model** (according to a metric, say highest validation Dice) and also save periodic checkpoints (every N epochs) in case you want to resume training or guard against crashes. Ensure the checkpoint includes the model state dict and optimizer state (and scheduler state) so you can resume seamlessly. If using PyTorch Lightning or a similar high-level framework, this is handled for you; but in a custom script, you’ll manually do `torch.save(model.state_dict(), filepath)` etc. for best model. Use the validation metrics to decide if the model improved.

- **Visualization & Debugging:** Apart from logging metrics, it’s very useful to visualize results during training. Your script could save sample outputs (images with predicted masks) to an output folder each epoch. Especially for vessels, watching a training sample’s output evolve can tell you if the model is learning fine details or just the big vessels. Also track if any instabilities occur (if loss becomes NaN, etc., incorporate checks and maybe automatically reduce LR or clip gradients if that happens).

To draw inspiration, you could look at the training script of the **MedSAM** project (Segment-Anything for Medical) by the same lab, which likely includes robust logging and checkpointing for large-scale training. MedSAM’s training (given it was done on multiple GPUs) would have logging of loss and validation at intervals, and model checkpoint saving via a slurm or distributed script [bowang-lab/MedSAM: Segment Anything in Medical Images - GitHub](https://github.com/bowang-lab/MedSAM#:~:text=bowang,to%20start%20the%20training%20process). Adapting those patterns, you want to ensure your U-Mamba training script is **configurable from the command line**.

### **Hyperparameter Tuning and CLI Flexibility:**

For research and practical use, it’s critical to make the training script flexible. Use a command-line argument parser (like Python’s `argparse`) to allow setting key hyperparameters without editing code. For instance, allow arguments for learning rate, number of epochs, batch size, whether to use U-Mamba_Enc or U-Mamba_Bot, whether to use deep supervision, etc. This makes experimentation much easier. Best practices for hyperparameter tuning are to start with the defaults (Dice+CE loss, ~0.01 LR for SGD or ~1e-3 for Adam, poly LR schedule, deep supervision on) and then adjust one thing at a time. You might do a grid or random search over learning rates (which often has the biggest impact). Also consider tuning the **loss weighting**: while by default Dice and BCE are unweighted (weight 1 each), some vessel segmentation tasks might benefit from a slight up-weighting of Dice loss if recall is low, or introducing a small additional loss. For example, one could add a **clDice loss** term (there are implementations available) to specifically encourage connectivity – tuning the weight of this term would be a hyperparameter to explore. Another hyperparameter is the **optimizer**: you could allow an option to choose Adam vs SGD. Adam might converge faster on vessels but sometimes can generalize slightly differently; having the option to try both is valuable. **Learning rate schedule** can be tuned as well – e.g., poly vs cosine annealing vs a fixed schedule. In vessel segmentation, some have found that a **warm restart or fine-tuning phase** focusing on hard examples can boost performance, so one could, for instance, train with a normal schedule then do a few epochs at a lower LR focusing on patches with the smallest vessels (this is an advanced strategy beyond basic training).

Your CLI should also accept paths for data and output, and perhaps an argument to resume from a checkpoint (to continue training or do fine-tuning). Logging verbosity could be adjustable (to turn on/off image saves, etc.). Essentially, treat your script as a **research training pipeline** where many components can be swapped or tuned. By doing so, you ensure that you can easily try, say, a different augmentation intensity or a different loss function, by just changing a command-line flag. This mirrors the flexibility of nnU-Net’s own command-line interface (where you specify trainer class, etc.). For example, you might run: `python train_umamba.py --train_dir data/train --val_dir data/val --epochs 500 --lr 0.005 --optimizer adam --deep_supervision True --variant enc` to train an encoder-variant U-Mamba with specified hyperparams. Good logging (to console and to a file) will help in analyzing these runs.

### **Adapting Existing Scripts:**

If you have a UNet training script, adapting it to U-Mamba mostly involves swapping the model and ensuring the loss is computed correctly for possibly multiple outputs. The rest (data loading, training loop) remains the same. For MedSAM’s script, one notable thing is that MedSAM likely deals with prompt inputs in addition to images; for U-Mamba segmentation you won’t have that, so the script can be simpler. But MedSAM’s handling of logging and checkpoints can be instructive – e.g., it likely uses a certain frequency to evaluate on validation and saves models, which you can emulate.

In summary, building a custom training script for U-Mamba involves: **initializing the model properly (preferably via the provided plan-based function)**, **using appropriate loss and optimizer settings** drawn from known best practices (Dice+BCE, etc.), and **including all the training loop essentials** (data augmentation, periodic evaluation, metric logging, saving checkpoints). By making the script modular and configurable, you can experiment with hyperparameters and find the best setup for your vessel segmentation task. And by borrowing patterns from nnU-Net (which is highly automated) and projects like MedSAM (which handle large-scale training), you can ensure your training runs are efficient, well-monitored, and reproducible.

# 5. Additional Research Sources and Community Insights

### **Official Repository and Documentation:**

The primary resource for U-Mamba is the official GitHub repository (bowang-lab/U-Mamba) and the accompanying paper [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=Convolutional%20Neural%20Networks%20,range%20dependency.%20Moreover). The repository integrates U-Mamba into the nnU-Net framework, which means much of its usage and configuration is documented through nnU-Net’s guidelines. The README and the project website [U-Mamba](https://u-mamba.github.io/#:~:text=Highlights) [U-Mamba](https://u-mamba.github.io/#:~:text=Network) highlight U-Mamba’s purpose: it’s a general-purpose 2D/3D biomedical segmentation network that combines CNN and state-space model components to handle both local details and long-range dependencies. The authors emphasize that U-Mamba _automatically adapts_ to different datasets (thanks to nnU-Net’s self-configuring nature) and achieves superior performance over CNN-based (like vanilla U-Net) and Transformer-based (like Swin UNETR) models across diverse tasks [U-Mamba](https://u-mamba.github.io/#:~:text=%2A%20We%20propose%20U,four%20diverse%20tasks%2C%20including%20the) [U-Mamba](https://u-mamba.github.io/#:~:text=Network). The official paper [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=Convolutional%20Neural%20Networks%20,range%20dependency.%20Moreover) is a great source for theoretical understanding – it provides background on SSM (the Mamba layer is based on a state-space ODE approach) [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=2,S6) [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=Structured%20State%20Space%20Sequence%20Models,by%20a%20significant%20margin) and explains why this is an appealing alternative to self-attention for long sequences (linear time complexity, etc.). For those looking to implement or modify U-Mamba, studying the code in `umamba/nnunetv2` directory is valuable: the network definitions and trainer classes illustrate how it inherits from nnU-Net. There is also a **BibTeX** and likely a link to a mailing list or discussions from the WangLab, indicating the developers were open to community engagement (so checking the Issues page on GitHub is worthwhile for troubleshooting).

### **Associated Research Papers:**

U-Mamba sits at the intersection of CNNs and state-space models in vision. The Mamba layer itself originates from a line of work on **S4 (Structured State Space) models**. Key references include the S4 model by Gu et al., and the Mamba architecture by Song et al. (which introduced the input-dependent selective mechanism) [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=Structured%20State%20Space%20Sequence%20Models,by%20a%20significant%20margin) [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=Recently%2C%20Mamba%C2%A0,notably%20simpler%20and%20has%20demonstrated). Those interested in the theory can refer to _“Mamba: Linear-Time Sequence Modeling with Selective State Spaces”_. For biomedical imaging specifically, there has been a surge of interest in applying state-space models: the paper introduces U-Mamba as one of the first in segmentation, but since its release, several related works have appeared. For instance, **Vision Mamba U-Net (VM-UNet)** and **Mamba-UNet** are names that appear in literature, essentially variations of combining U-Net with SSM layers [OCTAMamba: A State-Space Model Approach for Precision OCTA Vasculature Segmentation](https://arxiv.org/html/2409.08000v1#:~:text=Recently%2C%20structured%20state,UNet%20%5B19%5D%20is%20a%20novel). These works confirm that SSM-based models are competitive with Transformers for capturing long-range context in images [OCTAMamba: A State-Space Model Approach for Precision OCTA Vasculature Segmentation](https://arxiv.org/html/2409.08000v1#:~:text=Recently%2C%20structured%20state,UNet%20%5B19%5D%20is%20a%20novel). A recent paper “OCTAMamba” applied a Mamba-based U-Net to **OCTA vasculature segmentation**, showing that the approach is effective for retinal vessels [OCTAMamba: A State-Space Model Approach for Precision OCTA Vasculature Segmentation](https://arxiv.org/html/2409.08000v1#:~:text=Optical%20Coherence%20Tomography%20Angiography%20,scale%20vasculature%2C%20and%20a%20Focused) [OCTAMamba: A State-Space Model Approach for Precision OCTA Vasculature Segmentation](https://arxiv.org/html/2409.08000v1#:~:text=Recently%2C%20structured%20state,UNet%20%5B19%5D%20is%20a%20novel). Another paper, “Swin-UMamba,” combined ideas from Swin Transformer and U-Mamba [[PDF] Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining](https://papers.miccai.org/miccai-2024/paper/1627_paper.pdf#:~:text=pretraining%20papers,Net%20%5B11%5D%20and.%20Swin), indicating that hybrid architectures are being explored to push performance further.

In the context of **vessel segmentation research**, there are some notable works integrating shape knowledge. For example, a **“Tubular-Aware Mamba for Retinal Vessel Segmentation”** (as hinted by a research preprint title [[PDF] Tubular-Aware Mamba for Accurate Retinal Vessel Segmentation ...](https://assets-eu.researchsquare.com/files/rs-5164628/v1_covered_c765eeca-e5a9-49f4-b706-b3cdc7e0b87c.pdf#:~:text=%5BPDF%5D%20Tubular,Sensitive%20Network%20for%20Retinal)) presumably builds on U-Mamba by adding vessel-specific constraints or modules. And the **Serp-Mamba** paper specifically targets high-resolution ultra-wide-field retinal images with a modified scanning mechanism and an imbalance-focused module [Serp-Mamba: Advancing High-Resolution Retinal Vessel Segmentation with Selective State-Space Model](https://arxiv.org/html/2409.04356v1#:~:text=Space%20Model%20,transformations%2C%20ensures%20the%20effective%20and) [Serp-Mamba: Advancing High-Resolution Retinal Vessel Segmentation with Selective State-Space Model](https://arxiv.org/html/2409.04356v1#:~:text=and%20delicate%20nature%20of%20the,resolution%20images.%20Our%20ADDR%20module). These indicate that the community is actively building on U-Mamba for vessel segmentation, customizing it to handle extremely large images and tiny vessels. Such papers can provide inspiration on augmentations, loss functions, or post-processing unique to vessels. For example, Serp-Mamba’s SIA scan mechanism is a clever idea to follow curved vessels during processing [Serp-Mamba: Advancing High-Resolution Retinal Vessel Segmentation with Selective State-Space Model](https://arxiv.org/html/2409.04356v1#:~:text=Space%20Model%20,transformations%2C%20ensures%20the%20effective%20and), and their Ambiguity-Driven Dual Recalibration addresses class imbalance by adaptively thresholding predictions [Serp-Mamba: Advancing High-Resolution Retinal Vessel Segmentation with Selective State-Space Model](https://arxiv.org/html/2409.04356v1#:~:text=and%20delicate%20nature%20of%20the,resolution%20images.%20Our%20ADDR%20module). While these are custom innovations, they underscore common challenges in vessel segmentation and how one might extend U-Mamba.

### **Community Discussions and Issues:**

The U-Mamba GitHub’s Issues page is a valuable source of practical insight. One issue reported was the occurrence of **NaN losses during training** [Nan When training · Issue #8 · bowang-lab/U-Mamba · GitHub](https://github.com/bowang-lab/U-Mamba/issues/8#:~:text=Excellent%20work%20on%20the%20U,nnUNetTrainerUMambaEnc%20trainer%20for%20this%20process). The community (and the authors) identified that this was caused by using mixed precision (AMP) with the Mamba layer, and the resolution was to disable AMP – the authors even provided a new trainer class without AMP to address it [Nan When training · Issue #8 · bowang-lab/U-Mamba · GitHub](https://github.com/bowang-lab/U-Mamba/issues/8#:~:text=Thanks%20for%20your%20valuable%20feedback,AMP%20will%20lead%20to%20nan). This is a good example of how community feedback led to an improvement; it tells users that if they see instability, they should try full precision. Another issue pertained to the **nnU-Net plans compatibility** [The Plans_handler.py does not match the original repo. · Issue #62 · bowang-lab/U-Mamba · GitHub](https://github.com/bowang-lab/U-Mamba/issues/62#:~:text=I%27m%20using%20the%20offical,Why%20did%20you%20do%20this) [The Plans_handler.py does not match the original repo. · Issue #62 · bowang-lab/U-Mamba · GitHub](https://github.com/bowang-lab/U-Mamba/issues/62#:~:text=hjj194%20%20%20commented%20,71) – since U-Mamba extended nnUNet, there were some mismatches with plan file keys (e.g., some keys for kernel sizes were nested differently). Users discussed how to adjust the code or ensure they use the right nnU-Net version to avoid errors. This highlights that when using U-Mamba, one should use the exact nnU-Net version or code that comes with U-Mamba (the integration might not be 100% drop-in with vanilla nnU-Net v2 without minor tweaks). There have been questions on how to train U-Mamba on custom data, to which the general answer is to prepare the data in nnU-Net format and use the provided training commands (as given in the README) [U-Mamba/README.md at main · bowang-lab/U-Mamba · GitHub](https://github.com/bowang-lab/U-Mamba/blob/main/README.md#:~:text=Preprocessing) [U-Mamba/README.md at main · bowang-lab/U-Mamba · GitHub](https://github.com/bowang-lab/U-Mamba/blob/main/README.md#:~:text=%2A%20Train%202D%20%60U). The community has also explored extending U-Mamba: for instance, **UU-Mamba** (Uncertainty-aware U-Mamba) was proposed in an academic context to improve generalization on cardiac and vascular segmentation [Uncertainty-aware U-Mamba for Cardiac Image Segmentation](https://www.semanticscholar.org/paper/9575b1318a029cc09d51517b30713da7006f09a9#:~:text=Segmentation%20www,optimizer%20and%20an) [UU-Mamba: Uncertainty-aware U-Mamba for Cardiac Image Segmentation](https://arxiv.org/html/2405.17496v4#:~:text=errors%2C%20highlighting%20the%20need%20for,art%20models%20including). UU-Mamba combines U-Mamba with the **Sharpness-Aware Minimization (SAM) optimizer** and an uncertainty-based loss function [UU-Mamba: Uncertainty-aware U-Mamba for Cardiac Image Segmentation](https://arxiv.org/html/2405.17496v4#:~:text=errors%2C%20highlighting%20the%20need%20for,art%20models%20including). SAM is known to find flatter minima, which can improve generalization, and incorporating an uncertainty-aware loss means the network can model prediction uncertainty (helpful in medical imaging where data is limited). The fact that UU-Mamba outperformed other models on cardiac MRI suggests that these community-driven enhancements can be effective. From a practical standpoint, one might not implement SAM and uncertainty estimation initially, but it’s useful to know that these options exist and can be integrated if needed – especially if one finds the baseline U-Mamba is overfitting or not confident on certain vessel segments.

On forums like Reddit, discussions around Mamba and SSMs have taken place more generally. There was curiosity in the ML community about whether Mamba (the sequence model) would “replace transformers” given its theoretical speed advantages [[D] - Why MAMBA did not catch on? : r/MachineLearning](https://www.reddit.com/r/MachineLearning/comments/1hpg91o/d_why_mamba_did_not_catch_on/#:~:text=Discussion) [[D] - Why MAMBA did not catch on? : r/MachineLearning](https://www.reddit.com/r/MachineLearning/comments/1hpg91o/d_why_mamba_did_not_catch_on/#:~:text=Image%3A%20Profile%20Badge%20for%20the,Commenter). The consensus so far is that while Mamba/S4 models are very promising (and clearly U-Mamba’s success in segmentation is evidence of their power in specific domains), they have not yet achieved broad dominance over Transformers in all areas. Some Reddit commentary noted that in practice, the performance of trained Mamba models was similar to transformers, and the ecosystem/tooling around transformers is more mature [[D] - Why MAMBA did not catch on? : r/MachineLearning](https://www.reddit.com/r/MachineLearning/comments/1hpg91o/d_why_mamba_did_not_catch_on/#:~:text=Image%3A%20Profile%20Badge%20for%20the,Commenter). However, in niche domains like medical imaging, U-Mamba shows that a well-crafted SSM-CNN hybrid can beat transformers (Swin UNETR, etc.) in both quality and efficiency [U-Mamba](https://u-mamba.github.io/#:~:text=%2A%20We%20propose%20U,four%20diverse%20tasks%2C%20including%20the). The community interest is still high – evidenced by “Awesome Mamba” collections on GitHub [A Comprehensive Survey of State Space Models in Medical ... - GitHub](https://github.com/xmindflow/Awesome_Mamba#:~:text=This%20is%20a%20collection%20of,emphasis%20on%20Medical%20Image%20Analysis) and ongoing research. As new papers come out (like those mentioned above), it’s wise to keep an eye on how they might inform your implementation. For example, the **Metrics Reloaded** paper (cited in U-Mamba’s reference [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=Following%20the%20recommendations%20in%20Metrics,is%20an%20instance%20segmentation%20task)) is a comprehensive look at evaluation metrics; understanding such work can help in better evaluating vessel segmentation results beyond just Dice.

In conclusion, beyond the official U-Mamba sources, the **broader state-space model literature and the specialized vessel segmentation research** provide a context that can guide practical implementation decisions. The community’s experience – shared via GitHub issues and forums – emphasizes careful handling of precision and the benefits of certain optimizers (like SAM) and losses in specific cases. By leveraging these insights, one can implement U-Mamba for vessel segmentation more effectively: ensuring stable training, using appropriate metrics, and even extending the model with the latest techniques if needed. U-Mamba’s own strong results and the active community around it make it a promising backbone for vessel segmentation and other biomedical segmentation tasks moving forward.

## **Sources:**

U-Mamba paper and code [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=Image%3A%20Refer%20to%20caption%20,are%20presented%20in%20Table%C2%A0%2027) [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=U,a%201D%20convolutional%20layer%2C%20SiLU) [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=size%2C%20and%20network%20configurations%20%28e,2D%20and%203D%20datasets%2C%20respectively) [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=descent%20and%20the%20loss%20function,more%20streamlined%20and%20efficient%20evaluation), nnU-Net framework [U-Mamba Paper](http://ar5iv.org/abs/2401.04722#:~:text=comparison%20of%20U,also%20optimized%20with%20stochastic%20gradient), vessel segmentation literature [Serp-Mamba: Advancing High-Resolution Retinal Vessel Segmentation with Selective State-Space Model](https://arxiv.org/html/2409.04356v1#:~:text=Space%20Model%20,transformations%2C%20ensures%20the%20effective%20and) [Serp-Mamba: Advancing High-Resolution Retinal Vessel Segmentation with Selective State-Space Model](https://arxiv.org/html/2409.04356v1#:~:text=and%20delicate%20nature%20of%20the,resolution%20images.%20Our%20ADDR%20module) [Centerline Boundary Dice Loss for Vascular Segmentation | MICCAI 2024 - Open Access](https://papers.miccai.org/miccai-2024/129-Paper0458.html#:~:text=Vascular%20segmentation%20in%20medical%20imaging,aware), and community discussions on U-Mamba [Nan When training · Issue #8 · bowang-lab/U-Mamba · GitHub](https://github.com/bowang-lab/U-Mamba/issues/8#:~:text=Thanks%20for%20your%20valuable%20feedback,AMP%20will%20lead%20to%20nan).
