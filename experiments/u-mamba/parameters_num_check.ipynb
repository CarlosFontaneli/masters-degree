{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fonta42/Desktop/masters-degree/U-Mamba/umamba/nnunetv2/nets/UMambaEnc_2d.py:81: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled=False)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append('/home/fonta42/Desktop/masters-degree/U-Mamba')\n",
    "from umamba.nnunetv2.nets.UMambaEnc_2d import UMambaEnc\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters in U-Net: 32,521,105\n"
     ]
    }
   ],
   "source": [
    "unet_model = smp.Unet(\n",
    "        encoder_name=\"resnet50\",\n",
    "        encoder_weights=\"imagenet\",\n",
    "        in_channels=3,\n",
    "        classes=1,\n",
    ")\n",
    "\n",
    "num_params = count_parameters(unet_model)\n",
    "print(f\"Number of trainable parameters in U-Net: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_map_sizes: [[256, 256], [128, 128], [64, 64], [32, 32], [16, 16]]\n",
      "do_channel_token: [False, False, False, False, True]\n",
      "MambaLayer: dim: 64\n",
      "MambaLayer: dim: 256\n",
      "MambaLayer: dim: 256\n",
      "Number of trainable parameters in U-Mamba: 31,297,860\n"
     ]
    }
   ],
   "source": [
    "# Example configuration for a UMamba model aiming to match a ResNet-50 UNet\n",
    "# Define the input patch size as a tuple (height, width)\n",
    "input_size = (256, 256)           # The resolution of the input images (or patches) fed into the network\n",
    "\n",
    "# Define the number of input channels\n",
    "in_channels = 3                   # Number of channels in the input images; 3 for standard RGB images\n",
    "\n",
    "# Define the number of resolution stages in the encoder and decoder\n",
    "n_stages = 5                      # The network is structured into 5 stages (levels); each stage performs downsampling/upscaling\n",
    "\n",
    "# Define the number of feature channels for each stage\n",
    "features_per_stage = [64, 128, 256, 512, 1024]  \n",
    "# For each stage, the network will use these many channels. Increasing these values boosts model capacity.\n",
    "# Here, stage 1 has 64 channels, stage 2 has 128, and so on, with the deepest stage having 1024 channels.\n",
    "\n",
    "# Define the convolution kernel size for each stage\n",
    "kernel_sizes = [[3, 3]] * n_stages       \n",
    "# Each stage will use a 3x3 convolution kernel. This list is replicated for each stage.\n",
    "\n",
    "# Define the stride for each stage's downsampling operation\n",
    "strides = [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2]]\n",
    "# The first stage uses a stride of 1 (no downsampling), and each subsequent stage uses a stride of 2,\n",
    "# which halves the spatial dimensions at each stage.\n",
    "\n",
    "# Define the number of convolutional blocks per stage in the encoder\n",
    "n_conv_per_stage = 1              \n",
    "# Each stage in the encoder will have 1 convolutional block. Increasing this number will add more layers per stage.\n",
    "\n",
    "# Define the number of convolutional blocks per stage in the decoder\n",
    "n_conv_per_stage_decoder = 1       \n",
    "# Each stage in the decoder will have 1 convolutional block, mirroring the encoder structure.\n",
    "    \n",
    "num_classes = 1                   # Binary segmentation\n",
    "conv_op = nn.Conv2d              \n",
    "norm_op = nn.InstanceNorm2d      \n",
    "norm_op_kwargs = {'eps': 1e-5, 'affine': True}\n",
    "nonlin = nn.LeakyReLU            \n",
    "nonlin_kwargs = {'inplace': True}\n",
    "deep_supervision = False          # Use single output (to match SMP Unet)\n",
    "stem_channels = features_per_stage[0]\n",
    "\n",
    "# Instantiate UMambaEnc directly:\n",
    "\n",
    "umamba_model = UMambaEnc(\n",
    "    input_size=input_size,\n",
    "    input_channels=in_channels,\n",
    "    n_stages=n_stages,\n",
    "    features_per_stage=features_per_stage,\n",
    "    conv_op=conv_op,\n",
    "    kernel_sizes=kernel_sizes,\n",
    "    strides=strides,\n",
    "    n_conv_per_stage=n_conv_per_stage,\n",
    "    num_classes=num_classes,\n",
    "    n_conv_per_stage_decoder=n_conv_per_stage_decoder,\n",
    "    conv_bias=True,\n",
    "    norm_op=norm_op,\n",
    "    norm_op_kwargs=norm_op_kwargs,\n",
    "    dropout_op=None,\n",
    "    dropout_op_kwargs=None,\n",
    "    nonlin=nonlin,\n",
    "    nonlin_kwargs=nonlin_kwargs,\n",
    "    deep_supervision=deep_supervision,\n",
    "    stem_channels=stem_channels\n",
    ")\n",
    "\n",
    "num_params = count_parameters(umamba_model)\n",
    "print(f\"Number of trainable parameters in U-Mamba: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters in MedSAM: 93,729,252\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from segment_anything import sam_model_registry\n",
    "sys.path.append('../MedSAM')\n",
    "\n",
    "\n",
    "class MedSAM(nn.Module):\n",
    "    def __init__(self, sam_model):\n",
    "        super(MedSAM, self).__init__()\n",
    "        self.image_encoder = sam_model.image_encoder\n",
    "        self.mask_decoder = sam_model.mask_decoder\n",
    "        self.prompt_encoder = sam_model.prompt_encoder\n",
    "\n",
    "        for param in self.prompt_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, images, box):\n",
    "        image_embedding = self.image_encoder(images)\n",
    "        box_torch = torch.as_tensor(box, dtype=torch.float32, device=images.device)\n",
    "        if len(box_torch.shape) == 2:\n",
    "            box_torch = box_torch[:, None, :]\n",
    "\n",
    "        sparse_embeddings, dense_embeddings = self.prompt_encoder(\n",
    "            points=None, boxes=box_torch, masks=None\n",
    "        )\n",
    "\n",
    "        low_res_masks, _ = self.mask_decoder(\n",
    "            image_embeddings=image_embedding,\n",
    "            image_pe=self.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings,\n",
    "            dense_prompt_embeddings=dense_embeddings,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "\n",
    "        ori_res_masks = F.interpolate(\n",
    "            low_res_masks,\n",
    "            size=(images.shape[2], images.shape[3]),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        return ori_res_masks\n",
    "\n",
    "# Load the MedSAM model\n",
    "MedSAM_CKPT_PATH = \"/home/fonta42/Desktop/masters-degree/MedSAM/work_dir/MedSAM/medsam_vit_b.pth\"\n",
    "sam_model_inst = sam_model_registry['vit_b'](checkpoint=MedSAM_CKPT_PATH)\n",
    "medsam_model = MedSAM(sam_model_inst)\n",
    "\n",
    "# Count and print the number of parameters\n",
    "num_params = count_parameters(medsam_model)\n",
    "print(f\"Number of trainable parameters in MedSAM: {num_params:,}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (umamba)",
   "language": "python",
   "name": "umamba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
